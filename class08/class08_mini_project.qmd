---
title: "class08 Mini Project"
author: "Leah Johnson PID: A17394690"
format: pdf
toc: true
---

## Background

In today's class we will apply the methods and techniques clustering and PCA to help make sense of a real world breast cancer fine needle aspiration (FNA) biopsy

## Data import

We start by importing the data. It is a csv file so we will use the 'read.csv()' function.

```{r}
fna.data <- "WisconsinCancer.csv"
```

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
wisc.data <- read.csv(fna.data, row.names=1)
```

```{r}
head(wisc.data)
```

We make sure to remove the first 'diagnosis' column - I don't want to use this for my machine learning models. We will use it later to compare our results to the expert diagnosis.

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```


> Q1. How many observations are in the dataset?

```{r}
nrow(wisc.data)
```
569 observations

> Q2. How many observations have a malignant diagnosis?

```{r}
table(diagnosis)
sum(wisc.df$diagnosis == "M")
```
212 observations have a malignant diagnosis.

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length( grep("_mean", colnames(wisc.df)) )
```
10 variables in the data are suffixed with _mean.


## Principal Component Analysis (PCA)

The main function here is 'prcomp()' and we want to make sure we set the optional argument 'scale=TRUE':

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```


> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

PC1 captures 44.27% of the original variance.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the original variance in the data.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance in the data.

# Create a 'biplot()' of wisc.pr

```{r}
biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

I notice that the row names are the labels for each of the points, therefore there is lots of overlapping text making the data difficult to visualize. 

Our main PCA "score plot" or "PC plot" of results:

```{r}
library(ggplot2)
```
```{r}
ggplot(wisc.pr$x) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
Each point represents each benign or malignant sample and its cell characteristics.

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

I notice that majority of the points on the PC1 and PC3 plot are placed lower on the graph, the shape is very similar to the PC1 vs. PC2 graph and almost visually looks like a horizontally flipped version of the PC1 and PC2 graph.

```{r}
ggplot(wisc.pr$x) + 
  aes(PC1, PC3, col=diagnosis) + 
  geom_point()
```


# Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

"wisc.pr$rotation[,1]" will tell us which of the original feature measurements contribute most to PC1. There are potential features with larger contributions than this one. 

```{r}
summary(wisc.pr$rotation["concave.points_mean",1])
```

```{r}
summary(wisc.pr$rotation["concave.points_worst",2])
```


## Hierarchical Clustering

```{r}
data.scaled <- scale(wisc.data)
```


```{r}
data.dist <- dist(data.scaled)
```


```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```


```{r}
plot(wisc.hclust)
abline(wisc.data, col="red", lty=2)
```

```{r}
wisc.hclust <- hclust (dist( scale(wisc.data) ) )
plot(wisc.hclust)
```
```{r}
plot(wisc.hclust)
abline(h=20, col="red", lty=2)
```

You can also use the 'cutree()' function with an argument of 'k=4' rather than 'h=height'

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters)
```


## Clustering PCA results

Here we will take our PCA results and use those as input for clustering. In other words our 'wisc.pr$x' scores that we plotted above (the main output from PCA - how the data lie on our new principal component axis/variables) and use a subset of PCs as input for 'hclust()'

```{r}
pc.dist <- dist( wisc.pr$x[,1:3] )
wisc.pr.hclust <- hclust(pc.dist, method="ward.D2")
plot(wisc.pr.hclust)
```

> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

"ward.D2" is my favorite for obtaining the same results for the same data.dist dataset because it organizes the clusters to make the dendrogram clearer by decreasing within-group, or within-cluster, variance. 

Cut the dendrogram/tree into two main groups/clusters:

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

I want to know how the clustering into 'grps' with values of 1 or 2 correspond the expert 'diagnosis'

```{r}
table(grps, diagnosis)
```

My clustering **group 1** are mostly "M" diagnosis (179) and my clustering **group 2** are mostly "B" diagnosis.

24 False positives (FP)
179 True positives (TP)
333 True negatives (TN)
33 False negatives (FN)

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC2) +
  geom_point(col=grps)
```

> Q13. How well does the newly created hclust model with two clusters separate out the two “M” and “B” diagnoses?

There is greater separation between "M" and "B" diagnoses with 2 clusters.

```{r}
d <- dist(wisc.pr$x[, 1:7])
wisc.pr.hclust <- hclust(d, method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.

Prior to PCA, the separation of the diagnoses was not as concise in that there were four clusters to consider. After PCA, there were 2 clusters to display the greatest variation, or the greatest contributors to both benign and malignant tumors.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

## Prediction

> Q16. Which of these new patients should we prioritize for follow up based on your results?

Based on the new results from the new data, we should prioritize patient 2 for follow up since they fall closer to the points representing the "M" or malignancy chunk displayed in the plot.









